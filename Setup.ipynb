{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443794d7-1c30-41f4-95de-500abd1b3219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[773, 546, 975, 55, 797]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark  \n",
    "sc = pyspark.SparkContext('local[*]') \n",
    "# do something to prove it works \n",
    "rdd = sc.parallelize(range(1000)) \n",
    "rdd.takeSample(False, 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d219a38-984d-462a-8ed6-5fd8f42f2308",
   "metadata": {},
   "source": [
    "Write a function to format the file in a way that each appropriate value will correspond to only one column. If the data itself contains the splitter comma, it should be quoted by double quotes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9309373-987c-42ee-bae5-72bba86aa84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+--------+------------+--------------------+\n",
      "|order_id|delivery_company|quantity|   price|ordered_date|             address|\n",
      "+--------+----------------+--------+--------+------------+--------------------+\n",
      "|       1| delivery_comp_1|       1| 245. 52|    9-2-2022| Cedar Lane Houst...|\n",
      "|       2| delivery_comp_2|       2|  114.77|        null|Main Street,New Y...|\n",
      "|       3| delivery_comp_3|       0|  739.43|   14-3-2022|Main Street,Chica...|\n",
      "|       4| delivery_comp_0|       1|  878.93|   20/4/2022|                null|\n",
      "|       5| delivery_comp_1|       2|  481.44|        null|Maple Drive Chica...|\n",
      "|       6| delivery_comp_2|       0|   78.13|        null|Main Street,Houst...|\n",
      "|       7| delivery_comp_3|       1|  832.17|   20-2-2022|                null|\n",
      "|       8| delivery_comp_0|       2|   687.8|    1/4/2022|Maple Drive,Los A...|\n",
      "|       9| delivery_comp_1|       0|  338.44|   13/4/2022|Cedar Lane Miami,...|\n",
      "|      10| delivery_comp_2|       1|  461.33|        null|    Chicago,NY 77001|\n",
      "|      11| delivery_comp_3|       2|  544.33|    8/4/2022|Los Angeles,CA 90001|\n",
      "|      12| delivery_comp_0|       0|  200.20|        null|Cedar Lane,Los An...|\n",
      "|      13| delivery_comp_1|       1|  939.99|        null| Main Street,Chicago|\n",
      "|      14| delivery_comp_2|       2|   72.69|    2/3/2022|Oak Avenue,Los An...|\n",
      "|      15| delivery_comp_3|       0|  635.28|   15-2-2022|      Miami,FL 10001|\n",
      "|      16| delivery_comp_0|       1|  592.83|    2/3/2022|      Los Angeles,FL|\n",
      "|      17| delivery_comp_1|       2|   533.6|    2-3-2022|    Houston,TX 90001|\n",
      "|      18| delivery_comp_2|       0|  424.70|        null|   New York,TX 33101|\n",
      "|      19| delivery_comp_3|       1|  889.97|    4-3-2022|Elm Street,Miami ...|\n",
      "|      20| delivery_comp_0|       2|  222.12|   20/3/2022|      Miami,IL 77001|\n",
      "+--------+----------------+--------+--------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, regexp_replace, split, concat, when,expr\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def get_format_file():\n",
    "    # Read the text file\n",
    "    file_read = spark.read.text(\"clean_me.csv\")\n",
    "    # Format changues\n",
    "    splited_df_1 = file_read.withColumn(\"splited_colum\",split(file_read[\"value\"],\",\"))\n",
    "    splited_df_1 = splited_df_1.withColumn(\"order_id\", col(\"splited_colum\")[0])\n",
    "    splited_df_1 = splited_df_1.withColumn(\"delivery_company\", col(\"splited_colum\")[1])\n",
    "    splited_df_1 = splited_df_1.withColumn(\"quantity\", when(col(\"splited_colum\")[2].cast(\"int\").isNotNull(),\n",
    "                                                            col(\"splited_colum\")[2]) \\\n",
    "                                                            .otherwise(lit(0))) \n",
    "    splited_df_1 = splited_df_1.withColumn(\"price\",  when(col(\"splited_colum\")[4].cast(\"int\").isNotNull(),\n",
    "                                                        concat(col(\"splited_colum\")[3],lit(\".\"),col(\"splited_colum\")[4])) \n",
    "                                                        .otherwise(col(\"splited_colum\")[3])) \n",
    "    splited_df_1 = splited_df_1.withColumn(\"ordered_date\", when(col(\"splited_colum\")[4].cast(\"int\").isNotNull(),\n",
    "                                                        concat(col(\"splited_colum\")[5])) \\\n",
    "                                                        .otherwise(col(\"splited_colum\")[4]))\n",
    "    \n",
    "    splited_df_1 = splited_df_1.withColumn(\"address\", concat(col(\"splited_colum\")[6],lit(\",\"),col(\"splited_colum\")[7]))\n",
    "    # Delete first record that was as header\n",
    "    final_df = splited_df_1.select(\"order_id\",\"delivery_company\",\"quantity\",\"price\",\"ordered_date\",\"address\") \\\n",
    "        .filter(col(\"order_id\") != \"order_id\")\n",
    "    # Format changues\n",
    "    return final_df\n",
    "    \n",
    "# Call the function to execute the code\n",
    "get_format_file().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cb8ef-9d92-4987-b6c4-900c747ceca5",
   "metadata": {},
   "source": [
    "Write a function to unify column data values, ensure that: \n",
    "* Prices are truly double. \n",
    "* Order dates have the dd-MM-yyyy format. \n",
    "* Addresses have the $street, $city, $state, and $zipCode format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "164fff7b-d323-4047-9e72-c2ef2e88aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+------+------------+--------------------+\n",
      "|order_id|delivery_company|quantity| price|ordered_date|             address|\n",
      "+--------+----------------+--------+------+------------+--------------------+\n",
      "|       1| delivery_comp_1|       1|  null|        null|                null|\n",
      "|       2| delivery_comp_2|       2|114.77|        null|                null|\n",
      "|       3| delivery_comp_3|       0|739.43|  14-03-2022|Main Street,Chica...|\n",
      "|       4| delivery_comp_0|       1|878.93|  20-04-2022|                null|\n",
      "|       5| delivery_comp_1|       2|481.44|        null|Maple Drive,Chica...|\n",
      "|       6| delivery_comp_2|       0| 78.13|        null|Main Street,Houst...|\n",
      "|       7| delivery_comp_3|       1|832.17|  20-02-2022|                null|\n",
      "|       8| delivery_comp_0|       2| 687.8|  01-04-2022|Maple,Drive,Los,A...|\n",
      "|       9| delivery_comp_1|       0|338.44|  13-04-2022|Cedar Lane,Miami,...|\n",
      "|      10| delivery_comp_2|       1|461.33|        null|    Chicago,NY,77001|\n",
      "|      11| delivery_comp_3|       2|544.33|  08-04-2022|Los,Angeles,CA,90001|\n",
      "|      12| delivery_comp_0|       0| 200.2|        null|                null|\n",
      "|      13| delivery_comp_1|       1|939.99|        null| Main,Street,Chicago|\n",
      "|      14| delivery_comp_2|       2| 72.69|  02-03-2022|Oak,Avenue,Los,An...|\n",
      "|      15| delivery_comp_3|       0|635.28|  15-02-2022|      Miami,FL,10001|\n",
      "|      16| delivery_comp_0|       1|592.83|  02-03-2022|      Los,Angeles,FL|\n",
      "|      17| delivery_comp_1|       2| 533.6|  02-03-2022|    Houston,TX,90001|\n",
      "|      18| delivery_comp_2|       0| 424.7|        null|   New,York,TX,33101|\n",
      "|      19| delivery_comp_3|       1|889.97|  04-03-2022|Elm Street,Miami,...|\n",
      "|      20| delivery_comp_0|       2|222.12|  20-03-2022|      Miami,IL,77001|\n",
      "+--------+----------------+--------+------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import date_format,col,size\n",
    "\n",
    "data_set = get_format_file()\n",
    "\n",
    "def get_format_address(value):\n",
    "    # Method that formats addresses\n",
    "    split_chars = [\" \",\",\"]\n",
    "    \n",
    "    column_splited = split(value,\"[\"+\"\".join(split_chars)+\"]\")\n",
    " \n",
    "    splited = when(size(column_splited)== 5, \n",
    "                   concat(column_splited[0],\n",
    "                   lit(\" \"),\n",
    "                   column_splited[1],\n",
    "                   lit(\",\"),\n",
    "                   column_splited[2],\n",
    "                   lit(\",\"),\n",
    "                   column_splited[3],\n",
    "                   lit(\",\"),\n",
    "                   column_splited[4])) \\\n",
    "    .otherwise(when(size(column_splited)== 4, \n",
    "                   concat(column_splited[0],\n",
    "                   lit(\",\"),\n",
    "                   column_splited[1],\n",
    "                   lit(\",\"),\n",
    "                   column_splited[2],\n",
    "                   lit(\",\"),\n",
    "                   column_splited[3])) \\\n",
    "    .otherwise(when(size(column_splited)== 3, \n",
    "                   concat(\n",
    "                   column_splited[0],\n",
    "                   lit(\",\"),\n",
    "                   column_splited[1],\n",
    "                   lit(\",\"),\n",
    "                   column_splited[2])) \\\n",
    "    .otherwise(when(size(column_splited)== 2, \n",
    "                   concat(\n",
    "                   column_splited[0],\n",
    "                   lit(\",\"),\n",
    "                   column_splited[1])))))    \n",
    "    return splited\n",
    "\n",
    "    \n",
    "def get_format_date(value):\n",
    "    # format to handle date formats\n",
    "    splited = when(value.like(\"%-%\"), split(value,\"-\")).otherwise(split(value,\"/\"))\n",
    "    return concat(splited[2],lit(\"-\"),splited[1],lit(\"-\"),splited[0])\n",
    "\n",
    "\n",
    "def cast_prices_double(data_set):\n",
    "    # format to cast double \n",
    "    return data_set.withColumn(\"price\",col(\"price\").cast(\"double\"))\n",
    "\n",
    "def cast_date_format(data_set):\n",
    "    # format to handle date formats \n",
    "    return data_set.withColumn(\"ordered_date\",\n",
    "                               date_format(get_format_date(col(\"ordered_date\")),\"dd-MM-yyyy\"))\n",
    "\n",
    "def cast_address_format(data_set):\n",
    "    return data_set.withColumn(\"address\", get_format_address(col(\"address\")))\n",
    "\n",
    "\n",
    "def get_final_format():\n",
    "    # method that applies all transformations\n",
    "    data_set_1 = cast_prices_double(data_set)\n",
    "    \n",
    "    data_set_2 = cast_date_format(data_set_1)\n",
    "    \n",
    "    data_set_3 = cast_address_format(data_set_2)\n",
    "    \n",
    "    return data_set_3\n",
    "\n",
    "\n",
    "\n",
    "get_final_format().show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804fa43-8c7b-4b1a-b728-3c1aef54cf7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
